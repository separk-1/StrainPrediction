{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Import Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File â€œStrainTemperature.csvâ€ contains a dataset, with 7 columns and 337 rows.  \n",
    "- Col. 1 lists the strain measured in a structural member, in micro-strains, Î¼Îµ (i.e., in parts per million).  \n",
    "- Col. 2 to 7 list the temperature measured by 6 thermometers in different locations, in degree Celsius.  \n",
    "Each row refers to a specific time when all measures (of strain and temperature) are collected.\n",
    "Measures are collected every 30 minutes for one week (hence the rows are 337 = 7 Ã— 24 Ã— 2 + 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Strain   Temp1   Temp2   Temp3   Temp4   Temp5   Temp6\n",
      "0   69.754  22.681  23.836  24.512  25.141  23.650  24.048\n",
      "1   98.703  23.317  24.357  25.073  25.689  24.205  24.622\n",
      "2  104.404  23.945  24.926  25.564  26.028  24.741  25.121\n",
      "3  101.514  24.226  25.503  25.994  26.164  25.146  25.481\n",
      "4   99.808  24.432  26.114  26.177  26.272  25.443  25.765\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load data with the correct header\n",
    "data = pd.read_csv(\"./data/StrainTemperature.csv\", header=None)\n",
    "\n",
    "# Rename columns \n",
    "data.columns = ['Strain', 'Temp1', 'Temp2', 'Temp3', 'Temp4', 'Temp5', 'Temp6']\n",
    "\n",
    "# Check the structure of the corrected data\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Data Proprecessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for missing values and remove them if present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strain    0\n",
      "Temp1     0\n",
      "Temp2     0\n",
      "Temp3     0\n",
      "Temp4     0\n",
      "Temp5     0\n",
      "Temp6     0\n",
      "dtype: int64\n",
      "No missing values found.\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "missing_values = data.isnull().sum()\n",
    "print(missing_values)\n",
    "\n",
    "# Remove missing values if any are present\n",
    "if missing_values.sum() > 0:\n",
    "    data = data.dropna()  # Remove rows with missing values\n",
    "    print(\"Missing values have been handled.\")\n",
    "else:\n",
    "    print(\"No missing values found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of the missing value check shows no missing values. Therefore, the data remains unchanged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Calibrate a linear regression model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using all six temperatures and a â€œconstant feature,â€ infer the strain as a function of the temperatures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 Score: 0.9221524043718972\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Separate input (X) and output (y)\n",
    "X = data.iloc[:, 1:]  # Temperature data (columns 2 to 7)\n",
    "y = data.iloc[:, 0]   # Strain data (column 1)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions and evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"R^2 Score:\", r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) **Coefficient of Determination (ğ‘…Â²) for this Fitted Model**  \n",
    "The coefficient of determination (ğ‘…Â²) for this fitted model is **0.9222**.  \n",
    "This indicates that approximately **92.22% of the variability** in the test data is explained by this linear regression model.\n",
    "\n",
    "2) **How ğ‘…Â² Relates to the Uncertainty in Inferring the Strain**  \n",
    "The ğ‘…Â² value represents how well the model explains the relationship between the input data (temperatures) and the output data (strain).  \n",
    "\n",
    "    - A **high ğ‘…Â² value** (close to 1) suggests that the model effectively explains the variability in the data, resulting in **lower uncertainty** in predictions.  \n",
    "    - A **low ğ‘…Â² value** indicates that the model does not sufficiently explain the data, leading to **higher uncertainty** in predictions.\n",
    "\n",
    "3) **Conclusion**  \n",
    "With an ğ‘…Â² value of **0.9222**, this model effectively infers strain as a function of temperatures, showing relatively **low uncertainty**.  \n",
    "However, it is important to note that ğ‘…Â² alone does not fully evaluate model quality. Additional analyses, such as residual analysis and checks for multicollinearity, should also be considered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Build a 95% confidence interval**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a 95% confidence interval for each of the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                 Strain   R-squared:                       0.932\n",
      "Model:                            OLS   Adj. R-squared:                  0.931\n",
      "Method:                 Least Squares   F-statistic:                     754.7\n",
      "Date:                Tue, 07 Jan 2025   Prob (F-statistic):          2.31e-189\n",
      "Time:                        12:25:34   Log-Likelihood:                -1484.2\n",
      "No. Observations:                 337   AIC:                             2982.\n",
      "Df Residuals:                     330   BIC:                             3009.\n",
      "Df Model:                           6                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const       -319.8254     14.588    -21.923      0.000    -348.523    -291.127\n",
      "Temp1          8.1918     11.220      0.730      0.466     -13.881      30.264\n",
      "Temp2        -15.6028     11.590     -1.346      0.179     -38.402       7.197\n",
      "Temp3         -7.0257     11.657     -0.603      0.547     -29.957      15.906\n",
      "Temp4         -2.9973      9.559     -0.314      0.754     -21.802      15.807\n",
      "Temp5        -77.7885     16.772     -4.638      0.000    -110.781     -44.795\n",
      "Temp6        111.1475     37.323      2.978      0.003      37.726     184.569\n",
      "==============================================================================\n",
      "Omnibus:                        3.107   Durbin-Watson:                   0.599\n",
      "Prob(Omnibus):                  0.211   Jarque-Bera (JB):                2.931\n",
      "Skew:                           0.167   Prob(JB):                        0.231\n",
      "Kurtosis:                       2.688   Cond. No.                     1.96e+03\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 1.96e+03. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n",
      "\n",
      "95% Confidence Intervals:\n",
      "          Lower Bound  Upper Bound\n",
      "Constant  -348.523356  -291.127353\n",
      "Temp1      -13.880833    30.264428\n",
      "Temp2      -38.402249     7.196596\n",
      "Temp3      -29.957178    15.905853\n",
      "Temp4      -21.801958    15.807411\n",
      "Temp5     -110.781496   -44.795432\n",
      "Temp6       37.726259   184.568736\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import pandas as pd\n",
    "\n",
    "# Prepare data (set independent and dependent variables)\n",
    "X = data.iloc[:, 1:]  # Temperature data (columns 2 to 7)\n",
    "y = data.iloc[:, 0]   # Strain data (column 1)\n",
    "\n",
    "# Add a constant term (Statsmodels requires manual addition of the constant term)\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Fit the linear regression model\n",
    "model = sm.OLS(y, X).fit()\n",
    "\n",
    "# Display the results\n",
    "print(model.summary())\n",
    "\n",
    "# Extract 95% confidence intervals\n",
    "confidence_intervals = model.conf_int(alpha=0.05)  # 95% confidence intervals\n",
    "confidence_intervals.columns = ['Lower Bound', 'Upper Bound']\n",
    "confidence_intervals.index = ['Constant'] + list(data.columns[1:])  # Add variable names\n",
    "\n",
    "print(\"\\n95% Confidence Intervals:\")\n",
    "print(confidence_intervals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) **95% Confidence Intervals**  \n",
    "The 95% confidence intervals for each parameter representing the relationship between temperature and strain are shown in the last two columns [0.025, 0.975] of the results.  \n",
    "\n",
    "    **Examples:**   \n",
    "    - Constant (const) confidence interval: **[-348.523, -291.127]**  \n",
    "    - First variable (Temp1): **[-13.881, 30.264]**  \n",
    "    - Last variable (Temp6): **[37.726, 184.569]** \n",
    "     \n",
    "    The meaning of these intervals is that the true coefficient is expected to lie within these ranges with 95% confidence.\n",
    "\n",
    "\n",
    "2) **Significance of Coefficients**  \n",
    "    The `P>|t|` values indicate whether each variableâ€™s coefficient is statistically significant.  \n",
    "\n",
    "    **Typically:**  \n",
    "    - A **p-value < 0.05** implies the coefficient is statistically significant.\n",
    "\n",
    "    **Analysis of Results:**  \n",
    "    - Constant (const): **p=0.000**, significant.  \n",
    "    - Temp5: **p=0.000**, significant.  \n",
    "    - Temp6: **p=0.003**, significant.  \n",
    "    - Other variables (Temp1, Temp2, Temp3, Temp4): **p > 0.05**, not significant.\n",
    "\n",
    "\n",
    "3) **Size of Confidence Intervals**  \n",
    "Large confidence intervals suggest greater uncertainty in the estimation of the coefficients.\n",
    "\n",
    "    **Examples:**  \n",
    "    - Temp1 (first variable): **[-13.881, 30.264]** has a wide interval, indicating that the model struggles to accurately estimate its effect.  \n",
    "    - Temp2: **[-38.402, 7.197]**, Temp3: **[-29.957, 15.906]**, and Temp4: **[-21.802, 15.807]** also exhibit wide intervals.  \n",
    "    - Even for Temp6, which is significant, the interval **[37.726, 184.569]** is relatively large, implying some degree of uncertainty.\n",
    "\n",
    "    The large intervals for all variables (including significant ones) suggest considerable uncertainty in the coefficient estimates.\n",
    "\n",
    "\n",
    "4) **Reasons for Large Confidence Intervals**  \n",
    "    - **Multicollinearity:** High correlation among independent variables makes it challenging to estimate individual coefficients reliably.  \n",
    "    - **Data Quality:** Small sample sizes or noisy data increase the uncertainty of estimates.  \n",
    "    - **Low Predictive Power:** Variables with limited influence on the dependent variable (strain) result in wider intervals.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5. Mmulticollinearity. Compute the Variance Inflation Factor (VIF)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the Variance Inflation Factor (VIF) for each of the temperatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance Inflation Factor (VIF):\n",
      "  Feature           VIF\n",
      "0   Temp1   1680.118023\n",
      "1   Temp2   2419.379388\n",
      "2   Temp3   3147.839906\n",
      "3   Temp4   1836.505518\n",
      "4   Temp5   4979.115281\n",
      "5   Temp6  25248.587883\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Prepare data (select temperature variables only)\n",
    "X = data.iloc[:, 1:]  # Temperature data (columns 2 to 7)\n",
    "X_columns = X.columns\n",
    "\n",
    "# Calculate VIF\n",
    "vif_values = []\n",
    "\n",
    "for i in range(X.shape[1]):\n",
    "    # Set the current variable (i) as the dependent variable and the rest as independent variables\n",
    "    y = X.iloc[:, i]\n",
    "    X_temp = X.drop(X.columns[i], axis=1)\n",
    "    \n",
    "    # Train the linear regression model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_temp, y)\n",
    "    \n",
    "    # Compute the coefficient of determination (R^2)\n",
    "    r_squared = model.score(X_temp, y)\n",
    "    \n",
    "    # Calculate VIF\n",
    "    vif = 1 / (1 - r_squared)\n",
    "    vif_values.append(vif)\n",
    "\n",
    "# Organize VIF values into a DataFrame\n",
    "vif_data = pd.DataFrame({\n",
    "    \"Feature\": X_columns,\n",
    "    \"VIF\": vif_values\n",
    "})\n",
    "\n",
    "# Print the results\n",
    "print(\"Variance Inflation Factor (VIF):\")\n",
    "print(vif_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VIF (Variance Inflation Factor) measures how much a variable is correlated with other independent variables. A high VIF indicates that the variable is not independent but redundant, as it shares significant information with others. Typically, VIF > 10 suggests a multicollinearity issue.  \n",
    "\n",
    "In this case, all temperature variables have extremely high VIFs, indicating that they are highly correlated and act as **redundant** features rather than independent ones. This redundancy makes it difficult to isolate the individual impact of each variable in the model, highlighting the need to address multicollinearity through techniques like feature removal, dimensionality reduction (e.g., PCA), or using regularization methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance Inflation Factor (VIF):\n",
      "  Variable            VIF\n",
      "0    Temp1   43276.786079\n",
      "1    Temp2   48045.142288\n",
      "2    Temp3   45966.427308\n",
      "3    Temp4   29682.584833\n",
      "4    Temp5   86096.084991\n",
      "5    Temp6  483266.304104\n"
     ]
    }
   ],
   "source": [
    "# Use library for verification\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import pandas as pd\n",
    "\n",
    "# Prepare data (select temperature variables only)\n",
    "X = data.iloc[:, 1:]  # Temperature data (columns 2 to 7)\n",
    "\n",
    "# Calculate VIF\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"Variable\"] = X.columns  # Variable names\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "\n",
    "# Print results\n",
    "print(\"Variance Inflation Factor (VIF):\")\n",
    "print(vif_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why the Results Differ When Using the Library**\n",
    "\n",
    "The difference in VIF results arises because the `variance_inflation_factor` function from `statsmodels` uses matrix operations for calculations, providing more stable and consistent values. In contrast, manual methods rely on regression models, which can be sensitive to scaling or numerical precision issues.\n",
    "\n",
    "Despite the difference in exact VIF values, both approaches clearly indicate a **multicollinearity issue**, as all VIF values are significantly above the threshold (VIF > 10). This confirms that the temperature variables are highly correlated and redundant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6. Solve the Issue of Multicollinearity**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Reasons to Address Multicollinearity\n",
    "\n",
    "Addressing multicollinearity is essential for several reasons, which are all relevant to the current analysis:\n",
    "\n",
    "1) **Improving the inference of strain**  \n",
    "   Multicollinearity prevents the model from accurately estimating the individual effects of independent variables due to high correlations among them. Resolving it improves the accuracy of strain inference and enhances predictive performance.\n",
    "\n",
    "2) **Obtaining a simpler model**  \n",
    "   Multicollinearity leads to redundant variables in the model, increasing complexity unnecessarily. Removing such variables or applying dimensionality reduction techniques simplifies the model while maintaining or even improving its performance.\n",
    "\n",
    "3) **Better understanding of the relationship between strain and specific temperatures**  \n",
    "   High multicollinearity obscures the individual impact of specific temperature variables on strain. Reducing multicollinearity enables clearer interpretation of how each temperature affects strain.\n",
    "\n",
    "4) **Reducing uncertainty in model parameters**  \n",
    "   Multicollinearity destabilizes parameter estimates, widening confidence intervals and lowering the reliability of the model. Addressing it reduces uncertainty in parameter estimation and improves model stability.\n",
    "\n",
    "In conclusion, resolving multicollinearity is crucial for enhancing model performance, improving interpretability, and ensuring the stability and reliability of parameter estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Techniques for Resolving Multicollinearity "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Subset of thermometers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VIF Before Subset Selection:\n",
      "  Feature            VIF\n",
      "0   Temp1   43276.786079\n",
      "1   Temp2   48045.142288\n",
      "2   Temp3   45966.427308\n",
      "3   Temp4   29682.584833\n",
      "4   Temp5   86096.084991\n",
      "5   Temp6  483266.304104\n",
      "\n",
      "VIF After Subset Selection:\n",
      "  Feature           VIF\n",
      "0   Temp1   8392.004819\n",
      "1   Temp2  13700.298040\n",
      "2   Temp3   9104.187107\n",
      "3   Temp4    346.453943\n",
      "4   Temp5  84527.631223\n",
      "\n",
      "R^2 Score After Subset Selection: 0.9999542061569101\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Function to calculate VIF\n",
    "def calculate_vif(X):\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"Feature\"] = X.columns\n",
    "    vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "    return vif_data\n",
    "\n",
    "# Initial VIF calculation\n",
    "print(\"VIF Before Subset Selection:\")\n",
    "vif_before = calculate_vif(X)\n",
    "print(vif_before)\n",
    "\n",
    "# Remove the variable with the highest VIF (e.g., \"Temp6\")\n",
    "X_subset = X.drop(\"Temp6\", axis=1)\n",
    "\n",
    "# New VIF calculation after subset selection\n",
    "print(\"\\nVIF After Subset Selection:\")\n",
    "vif_after = calculate_vif(X_subset)\n",
    "print(vif_after)\n",
    "\n",
    "# Train-test split with the reduced subset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_subset, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model with the reduced subset\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"\\nR^2 Score After Subset Selection:\", r2_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì™œ ë³€ìˆ˜ ì œê±°ë¥¼ í•˜ê¸°ë¡œ í–ˆëŠ”ì§€  \n",
    "ì–´ë–»ê²Œ í–ˆëŠ”ì§€  \n",
    "ê²°ê³¼ê³¼  \n",
    "í•œê³„: ë³€ìˆ˜ ì œê±°ëŠ” ê·¼ë³¸ì ì¸ í•´ê²°ì´ ì•„ë‹ ìˆ˜ ìˆìŒ. ì™œëƒí•˜ë©´..  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Principal Component Analysis (PCA)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "PCAë¥¼ ì‚¬ìš©í•´ ì°¨ì›ì„ ì¶•ì†Œí•˜ê³ , ë‹¤ì¤‘ê³µì„ ì„±ì„ ì œê±°í•œ ìƒˆë¡œìš´ ë³€ìˆ˜ë¡œ ëª¨ë¸ì„ í•™ìŠµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 Score After PCA: 0.999950731484512\n",
      "Explained Variance Ratio: [0.98406031 0.0133858 ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Standardizing the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Apply PCA (Principal Component Analysis) with 2 principal components\n",
    "# PCA is used to reduce dimensionality by transforming correlated variables into independent components.\n",
    "# Here, we use 2 principal components to retain the most significant variance while simplifying the dataset.\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Train-test split with the transformed PCA data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the regression model with PCA-transformed data\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions and evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"R^2 Score After PCA:\", r2_score(y_test, y_pred))\n",
    "\n",
    "# Output the explained variance ratio of the principal components\n",
    "# This shows how much variance is retained by each principal component.\n",
    "print(\"Explained Variance Ratio:\", pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì™œ PCAë¥¼ í•˜ê¸°ë¡œ í–ˆëŠ”ì§€  \n",
    "ì–´ë–»ê²Œ í–ˆëŠ”ì§€ (ì™œ ì£¼ì„±ë¶„ì´ 2ê°œì¸ì§€)  \n",
    "ê²°ê³¼ê³¼\n",
    "í•œê³„: ê·¼ë³¸ì ì¸ í•´ê²°ì´ ì•„ë‹ ìˆ˜ ìˆìŒ. ì™œëƒí•˜ë©´.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 Score with Ridge Regression: 0.9999515809826575\n",
      "Ridge Coefficients: [1.90755538 0.27665518]\n",
      "Mean Squared Error with Ridge Regression: 0.0010595567328021806\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Ridge regression model\n",
    "ridge = Ridge(alpha=1.0)\n",
    "ridge.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_ridge = ridge.predict(X_test)\n",
    "\n",
    "# R^2 score\n",
    "print(\"R^2 Score with Ridge Regression:\", r2_score(y_test, y_pred_ridge))\n",
    "\n",
    "# Coefficients\n",
    "print(\"Ridge Coefficients:\", ridge.coef_)\n",
    "\n",
    "# Mean squared error (MSE) comparison\n",
    "mse_ridge = mean_squared_error(y_test, y_pred_ridge)\n",
    "print(\"Mean Squared Error with Ridge Regression:\", mse_ridge)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì™œ ë¦¿ì§€íšŒê·€ë¥¼ ë¥¼ í•˜ê¸°ë¡œ í–ˆëŠ”ì§€  \n",
    "ì–´ë–»ê²Œ í–ˆëŠ”ì§€   \n",
    "ê²°ê³¼  \n",
    "í•œê³„: ê·¼ë³¸ì ì¸ í•´ê²°ì´ ì•„ë‹ ìˆ˜ ìˆìŒ. ì™œëƒí•˜ë©´.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 Score with Lasso Regression: 0.9996497233047609\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# ë¼ì˜ íšŒê·€ ëª¨ë¸\n",
    "lasso = Lasso(alpha=0.1)  # ì•ŒíŒŒ ê°’ì€ ì •ê·œí™” ê°•ë„ë¥¼ ì¡°ì ˆ\n",
    "lasso.fit(X_train, y_train)\n",
    "y_pred_lasso = lasso.predict(X_test)\n",
    "\n",
    "# R^2 ê²°ê³¼ ì¶œë ¥\n",
    "print(\"R^2 Score with Lasso Regression:\", r2_score(y_test, y_pred_lasso))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì™œ ë¼ì˜ì˜íšŒê·€ë¥¼ ë¥¼ í•˜ê¸°ë¡œ í–ˆëŠ”ì§€  \n",
    "ì–´ë–»ê²Œ í–ˆëŠ”ì§€   \n",
    "ê²°ê³¼  \n",
    "í•œê³„: ê·¼ë³¸ì ì¸ í•´ê²°ì´ ì•„ë‹ ìˆ˜ ìˆìŒ. ì™œëƒí•˜ë©´.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**ì „ì²´ ê²°ë¡ **\n",
    "1. **Subset Selection**:\n",
    "   - ë³€ìˆ˜ ì œê±°ë¡œ ë‹¤ì¤‘ê³µì„ ì„±ì„ ì™„í™”í•  ìˆ˜ ìˆìœ¼ë‚˜, ê°€ì¥ ê°„ë‹¨í•œ ì ‘ê·¼ë²•.\n",
    "   - ë³€ìˆ˜ ì œê±°ëŠ” ë°ì´í„° í•´ì„ì— ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆìŒ.\n",
    "\n",
    "2. **PCA**:\n",
    "   - ë‹¤ì¤‘ê³µì„ ì„±ì„ ì™„ì „íˆ ì œê±°í•˜ë©°, ëª¨ë¸ì˜ ì„¤ëª…ë ¥ì„ ìœ ì§€.\n",
    "   - ì°¨ì›ì„ ì¶•ì†Œí•˜ë¯€ë¡œ ëª¨ë¸ í•´ì„ë ¥ì´ ë–¨ì–´ì§ˆ ìˆ˜ ìˆìŒ.\n",
    "\n",
    "3. **Ridge Regression**:\n",
    "   - ë‹¤ì¤‘ê³µì„ ì„±ì„ í•´ê²°í•˜ë©´ì„œ ëª¨ë“  ë³€ìˆ˜ë¥¼ ìœ ì§€.\n",
    "   - í•´ì„ ê°€ëŠ¥ì„±ì´ ë†’ìŒ.\n",
    "\n",
    "4. **Lasso Regression**:\n",
    "   - ë‹¤ì¤‘ê³µì„ ì„± í•´ê²°ê³¼ ë³€ìˆ˜ ì„ íƒì„ ë™ì‹œì— ìˆ˜í–‰.\n",
    "   - ê°€ì¥ í•´ì„ ê°€ëŠ¥í•˜ê³  ê°„ë‹¨í•œ ëª¨ë¸ì„ ìƒì„±í•  ìˆ˜ ìˆìŒ.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **7. Predict Future**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ì„ í˜• íšŒê·€ë¥¼ ì‚¬ìš©í•˜ì—¬, í˜„ì¬ (ë˜ëŠ” ê³¼ê±°)ì˜ ì˜¨ë„ì™€ ë³€í˜•ë¥ ì„ í•¨ìˆ˜ë¡œ í•˜ì—¬ ë¯¸ë˜ì˜ ë³€í˜•ë¥ ì„ ì˜ˆì¸¡í•˜ëŠ” ëª¨ë¸ì„ ê°œë°œí•˜ì‹­ì‹œì˜¤. (ë¯¸ë˜ ì‹œì ì—ì„œ ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ì „ ì§ˆë¬¸ì—ì„œ ì œì•ˆí•œ ëª¨ë¸ê³¼ ê´€ë ¨í•˜ì—¬, ì˜ˆì¸¡ì˜ ì •í™•ë„ë¥¼ ì •ëŸ‰í™”í•˜ì‹­ì‹œì˜¤.\n",
    "ë¯¸ë˜ ë³€í˜•ë¥ ì— ëŒ€í•´ 95% ì‹ ë¢°êµ¬ê°„ì„ ì–´ë–»ê²Œ ì •ì˜í•  ìˆ˜ ìˆìŠµë‹ˆê¹Œ?\n",
    "ì´ë¥¼ ë‹µí•˜ê¸° ìœ„í•´, ì—°ì†ëœ ì‹œì ì—ì„œ ë³€í˜•ë¥ ì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ë…¸ì´ì¦ˆê°€ ìƒê´€ë˜ì–´ ìˆì„ ìˆ˜ ìˆìŒì„ ê³ ë ¤í•˜ì‹­ì‹œì˜¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë…¸ì´ì¦ˆ ìƒê´€ì„± ë¬¸ì œë¥¼ ë¶„ì„í•˜ê³  ì´ê²ƒì´ ì´ˆë˜í•˜ëŠ” ê²°ê³¼ë¥¼ ì„¤ëª…í•˜ë©°, ë¯¸ë˜ ë³€í˜•ë¥ ì— ëŒ€í•œ ì‹ ë¢°êµ¬ê°„ì„ ì •ì˜í•  ë•Œ ì´ í˜„ìƒì„ ì–´ë–»ê²Œ ë°˜ì˜í•  ìˆ˜ ìˆì„ì§€ ë…¼ì˜í•˜ì‹­ì‹œì˜¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë‹¤ì¤‘ê³µì„ ì„±ì´ ë§ˆì§€ë§‰ ë‘ ì§ˆë¬¸ì—ì„œ ì •ì˜ëœ ë¯¸ë˜ ë³€í˜•ë¥  ì˜ˆì¸¡ê³¼ ì–´ë–»ê²Œ ê´€ë ¨ë˜ì–´ ìˆëŠ”ì§€ ë…¼ì˜"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "StrainPrediction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
