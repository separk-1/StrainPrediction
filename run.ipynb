{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Import Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File “StrainTemperature.csv” contains a dataset, with 7 columns and 337 rows.  \n",
    "- Col. 1 lists the strain measured in a structural member, in micro-strains, με (i.e., in parts per million).  \n",
    "- Col. 2 to 7 list the temperature measured by 6 thermometers in different locations, in degree Celsius.  \n",
    "Each row refers to a specific time when all measures (of strain and temperature) are collected.\n",
    "Measures are collected every 30 minutes for one week (hence the rows are 337 = 7 × 24 × 2 + 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Strain   Temp1   Temp2   Temp3   Temp4   Temp5   Temp6\n",
      "0   69.754  22.681  23.836  24.512  25.141  23.650  24.048\n",
      "1   98.703  23.317  24.357  25.073  25.689  24.205  24.622\n",
      "2  104.404  23.945  24.926  25.564  26.028  24.741  25.121\n",
      "3  101.514  24.226  25.503  25.994  26.164  25.146  25.481\n",
      "4   99.808  24.432  26.114  26.177  26.272  25.443  25.765\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load data with the correct header\n",
    "data = pd.read_csv(\"./data/StrainTemperature.csv\", header=None)\n",
    "\n",
    "# Rename columns \n",
    "data.columns = ['Strain', 'Temp1', 'Temp2', 'Temp3', 'Temp4', 'Temp5', 'Temp6']\n",
    "\n",
    "# Check the structure of the corrected data\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Data Proprecessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for missing values and remove them if present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strain    0\n",
      "Temp1     0\n",
      "Temp2     0\n",
      "Temp3     0\n",
      "Temp4     0\n",
      "Temp5     0\n",
      "Temp6     0\n",
      "dtype: int64\n",
      "No missing values found.\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "missing_values = data.isnull().sum()\n",
    "print(missing_values)\n",
    "\n",
    "# Remove missing values if any are present\n",
    "if missing_values.sum() > 0:\n",
    "    data = data.dropna()  # Remove rows with missing values\n",
    "    print(\"Missing values have been handled.\")\n",
    "else:\n",
    "    print(\"No missing values found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of the missing value check shows no missing values. Therefore, the data remains unchanged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Calibrate a linear regression model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using all six temperatures and a “constant feature,” infer the strain as a function of the temperatures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 Score: 0.9221524043718972\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Separate input (X) and output (y)\n",
    "X = data.iloc[:, 1:]  # Temperature data (columns 2 to 7)\n",
    "y = data.iloc[:, 0]   # Strain data (column 1)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions and evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"R^2 Score:\", r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) **Coefficient of Determination (𝑅²) for this Fitted Model**  \n",
    "The coefficient of determination (𝑅²) for this fitted model is **0.9222**.  \n",
    "This indicates that approximately **92.22% of the variability** in the test data is explained by this linear regression model.\n",
    "\n",
    "2) **How 𝑅² Relates to the Uncertainty in Inferring the Strain**  \n",
    "The 𝑅² value represents how well the model explains the relationship between the input data (temperatures) and the output data (strain).  \n",
    "\n",
    "    - A **high 𝑅² value** (close to 1) suggests that the model effectively explains the variability in the data, resulting in **lower uncertainty** in predictions.  \n",
    "    - A **low 𝑅² value** indicates that the model does not sufficiently explain the data, leading to **higher uncertainty** in predictions.\n",
    "\n",
    "3) **Conclusion**  \n",
    "With an 𝑅² value of **0.9222**, this model effectively infers strain as a function of temperatures, showing relatively **low uncertainty**.  \n",
    "However, it is important to note that 𝑅² alone does not fully evaluate model quality. Additional analyses, such as residual analysis and checks for multicollinearity, should also be considered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Build a 95% confidence interval**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a 95% confidence interval for each of the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                 Strain   R-squared:                       0.932\n",
      "Model:                            OLS   Adj. R-squared:                  0.931\n",
      "Method:                 Least Squares   F-statistic:                     754.7\n",
      "Date:                Tue, 07 Jan 2025   Prob (F-statistic):          2.31e-189\n",
      "Time:                        12:25:34   Log-Likelihood:                -1484.2\n",
      "No. Observations:                 337   AIC:                             2982.\n",
      "Df Residuals:                     330   BIC:                             3009.\n",
      "Df Model:                           6                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const       -319.8254     14.588    -21.923      0.000    -348.523    -291.127\n",
      "Temp1          8.1918     11.220      0.730      0.466     -13.881      30.264\n",
      "Temp2        -15.6028     11.590     -1.346      0.179     -38.402       7.197\n",
      "Temp3         -7.0257     11.657     -0.603      0.547     -29.957      15.906\n",
      "Temp4         -2.9973      9.559     -0.314      0.754     -21.802      15.807\n",
      "Temp5        -77.7885     16.772     -4.638      0.000    -110.781     -44.795\n",
      "Temp6        111.1475     37.323      2.978      0.003      37.726     184.569\n",
      "==============================================================================\n",
      "Omnibus:                        3.107   Durbin-Watson:                   0.599\n",
      "Prob(Omnibus):                  0.211   Jarque-Bera (JB):                2.931\n",
      "Skew:                           0.167   Prob(JB):                        0.231\n",
      "Kurtosis:                       2.688   Cond. No.                     1.96e+03\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 1.96e+03. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n",
      "\n",
      "95% Confidence Intervals:\n",
      "          Lower Bound  Upper Bound\n",
      "Constant  -348.523356  -291.127353\n",
      "Temp1      -13.880833    30.264428\n",
      "Temp2      -38.402249     7.196596\n",
      "Temp3      -29.957178    15.905853\n",
      "Temp4      -21.801958    15.807411\n",
      "Temp5     -110.781496   -44.795432\n",
      "Temp6       37.726259   184.568736\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import pandas as pd\n",
    "\n",
    "# Prepare data (set independent and dependent variables)\n",
    "X = data.iloc[:, 1:]  # Temperature data (columns 2 to 7)\n",
    "y = data.iloc[:, 0]   # Strain data (column 1)\n",
    "\n",
    "# Add a constant term (Statsmodels requires manual addition of the constant term)\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Fit the linear regression model\n",
    "model = sm.OLS(y, X).fit()\n",
    "\n",
    "# Display the results\n",
    "print(model.summary())\n",
    "\n",
    "# Extract 95% confidence intervals\n",
    "confidence_intervals = model.conf_int(alpha=0.05)  # 95% confidence intervals\n",
    "confidence_intervals.columns = ['Lower Bound', 'Upper Bound']\n",
    "confidence_intervals.index = ['Constant'] + list(data.columns[1:])  # Add variable names\n",
    "\n",
    "print(\"\\n95% Confidence Intervals:\")\n",
    "print(confidence_intervals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) **95% Confidence Intervals**  \n",
    "The 95% confidence intervals for each parameter representing the relationship between temperature and strain are shown in the last two columns [0.025, 0.975] of the results.  \n",
    "\n",
    "    **Examples:**   \n",
    "    - Constant (const) confidence interval: **[-348.523, -291.127]**  \n",
    "    - First variable (Temp1): **[-13.881, 30.264]**  \n",
    "    - Last variable (Temp6): **[37.726, 184.569]** \n",
    "     \n",
    "    The meaning of these intervals is that the true coefficient is expected to lie within these ranges with 95% confidence.\n",
    "\n",
    "\n",
    "2) **Significance of Coefficients**  \n",
    "    The `P>|t|` values indicate whether each variable’s coefficient is statistically significant.  \n",
    "\n",
    "    **Typically:**  \n",
    "    - A **p-value < 0.05** implies the coefficient is statistically significant.\n",
    "\n",
    "    **Analysis of Results:**  \n",
    "    - Constant (const): **p=0.000**, significant.  \n",
    "    - Temp5: **p=0.000**, significant.  \n",
    "    - Temp6: **p=0.003**, significant.  \n",
    "    - Other variables (Temp1, Temp2, Temp3, Temp4): **p > 0.05**, not significant.\n",
    "\n",
    "\n",
    "3) **Size of Confidence Intervals**  \n",
    "Large confidence intervals suggest greater uncertainty in the estimation of the coefficients.\n",
    "\n",
    "    **Examples:**  \n",
    "    - Temp1 (first variable): **[-13.881, 30.264]** has a wide interval, indicating that the model struggles to accurately estimate its effect.  \n",
    "    - Temp2: **[-38.402, 7.197]**, Temp3: **[-29.957, 15.906]**, and Temp4: **[-21.802, 15.807]** also exhibit wide intervals.  \n",
    "    - Even for Temp6, which is significant, the interval **[37.726, 184.569]** is relatively large, implying some degree of uncertainty.\n",
    "\n",
    "    The large intervals for all variables (including significant ones) suggest considerable uncertainty in the coefficient estimates.\n",
    "\n",
    "\n",
    "4) **Reasons for Large Confidence Intervals**  \n",
    "    - **Multicollinearity:** High correlation among independent variables makes it challenging to estimate individual coefficients reliably.  \n",
    "    - **Data Quality:** Small sample sizes or noisy data increase the uncertainty of estimates.  \n",
    "    - **Low Predictive Power:** Variables with limited influence on the dependent variable (strain) result in wider intervals.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5. Mmulticollinearity. Compute the Variance Inflation Factor (VIF)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the Variance Inflation Factor (VIF) for each of the temperatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance Inflation Factor (VIF):\n",
      "  Feature           VIF\n",
      "0   Temp1   1680.118023\n",
      "1   Temp2   2419.379388\n",
      "2   Temp3   3147.839906\n",
      "3   Temp4   1836.505518\n",
      "4   Temp5   4979.115281\n",
      "5   Temp6  25248.587883\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Prepare data (select temperature variables only)\n",
    "X = data.iloc[:, 1:]  # Temperature data (columns 2 to 7)\n",
    "X_columns = X.columns\n",
    "\n",
    "# Calculate VIF\n",
    "vif_values = []\n",
    "\n",
    "for i in range(X.shape[1]):\n",
    "    # Set the current variable (i) as the dependent variable and the rest as independent variables\n",
    "    y = X.iloc[:, i]\n",
    "    X_temp = X.drop(X.columns[i], axis=1)\n",
    "    \n",
    "    # Train the linear regression model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_temp, y)\n",
    "    \n",
    "    # Compute the coefficient of determination (R^2)\n",
    "    r_squared = model.score(X_temp, y)\n",
    "    \n",
    "    # Calculate VIF\n",
    "    vif = 1 / (1 - r_squared)\n",
    "    vif_values.append(vif)\n",
    "\n",
    "# Organize VIF values into a DataFrame\n",
    "vif_data = pd.DataFrame({\n",
    "    \"Feature\": X_columns,\n",
    "    \"VIF\": vif_values\n",
    "})\n",
    "\n",
    "# Print the results\n",
    "print(\"Variance Inflation Factor (VIF):\")\n",
    "print(vif_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VIF (Variance Inflation Factor) measures how much a variable is correlated with other independent variables. A high VIF indicates that the variable is not independent but redundant, as it shares significant information with others. Typically, VIF > 10 suggests a multicollinearity issue.  \n",
    "\n",
    "In this case, all temperature variables have extremely high VIFs, indicating that they are highly correlated and act as **redundant** features rather than independent ones. This redundancy makes it difficult to isolate the individual impact of each variable in the model, highlighting the need to address multicollinearity through techniques like feature removal, dimensionality reduction (e.g., PCA), or using regularization methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance Inflation Factor (VIF):\n",
      "  Variable            VIF\n",
      "0    Temp1   43276.786079\n",
      "1    Temp2   48045.142288\n",
      "2    Temp3   45966.427308\n",
      "3    Temp4   29682.584833\n",
      "4    Temp5   86096.084991\n",
      "5    Temp6  483266.304104\n"
     ]
    }
   ],
   "source": [
    "# Use library for verification\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import pandas as pd\n",
    "\n",
    "# Prepare data (select temperature variables only)\n",
    "X = data.iloc[:, 1:]  # Temperature data (columns 2 to 7)\n",
    "\n",
    "# Calculate VIF\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"Variable\"] = X.columns  # Variable names\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "\n",
    "# Print results\n",
    "print(\"Variance Inflation Factor (VIF):\")\n",
    "print(vif_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why the Results Differ When Using the Library**\n",
    "\n",
    "The difference in VIF results arises because the `variance_inflation_factor` function from `statsmodels` uses matrix operations for calculations, providing more stable and consistent values. In contrast, manual methods rely on regression models, which can be sensitive to scaling or numerical precision issues.\n",
    "\n",
    "Despite the difference in exact VIF values, both approaches clearly indicate a **multicollinearity issue**, as all VIF values are significantly above the threshold (VIF > 10). This confirms that the temperature variables are highly correlated and redundant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6. Solve the Issue of Multicollinearity**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Reasons to Address Multicollinearity\n",
    "\n",
    "Addressing multicollinearity is essential for several reasons, which are all relevant to the current analysis:\n",
    "\n",
    "1) **Improving the inference of strain**  \n",
    "   Multicollinearity prevents the model from accurately estimating the individual effects of independent variables due to high correlations among them. Resolving it improves the accuracy of strain inference and enhances predictive performance.\n",
    "\n",
    "2) **Obtaining a simpler model**  \n",
    "   Multicollinearity leads to redundant variables in the model, increasing complexity unnecessarily. Removing such variables or applying dimensionality reduction techniques simplifies the model while maintaining or even improving its performance.\n",
    "\n",
    "3) **Better understanding of the relationship between strain and specific temperatures**  \n",
    "   High multicollinearity obscures the individual impact of specific temperature variables on strain. Reducing multicollinearity enables clearer interpretation of how each temperature affects strain.\n",
    "\n",
    "4) **Reducing uncertainty in model parameters**  \n",
    "   Multicollinearity destabilizes parameter estimates, widening confidence intervals and lowering the reliability of the model. Addressing it reduces uncertainty in parameter estimation and improves model stability.\n",
    "\n",
    "In conclusion, resolving multicollinearity is crucial for enhancing model performance, improving interpretability, and ensuring the stability and reliability of parameter estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Techniques for Resolving Multicollinearity "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Subset of thermometers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VIF Before Subset Selection:\n",
      "  Feature            VIF\n",
      "0   Temp1   43276.786079\n",
      "1   Temp2   48045.142288\n",
      "2   Temp3   45966.427308\n",
      "3   Temp4   29682.584833\n",
      "4   Temp5   86096.084991\n",
      "5   Temp6  483266.304104\n",
      "\n",
      "VIF After Subset Selection:\n",
      "  Feature           VIF\n",
      "0   Temp1   8392.004819\n",
      "1   Temp2  13700.298040\n",
      "2   Temp3   9104.187107\n",
      "3   Temp4    346.453943\n",
      "4   Temp5  84527.631223\n",
      "\n",
      "R^2 Score After Subset Selection: 0.9999542061569101\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Function to calculate VIF\n",
    "def calculate_vif(X):\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"Feature\"] = X.columns\n",
    "    vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "    return vif_data\n",
    "\n",
    "# Initial VIF calculation\n",
    "print(\"VIF Before Subset Selection:\")\n",
    "vif_before = calculate_vif(X)\n",
    "print(vif_before)\n",
    "\n",
    "# Remove the variable with the highest VIF (e.g., \"Temp6\")\n",
    "X_subset = X.drop(\"Temp6\", axis=1)\n",
    "\n",
    "# New VIF calculation after subset selection\n",
    "print(\"\\nVIF After Subset Selection:\")\n",
    "vif_after = calculate_vif(X_subset)\n",
    "print(vif_after)\n",
    "\n",
    "# Train-test split with the reduced subset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_subset, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model with the reduced subset\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"\\nR^2 Score After Subset Selection:\", r2_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "왜 변수 제거를 하기로 했는지  \n",
    "어떻게 했는지  \n",
    "결과과  \n",
    "한계: 변수 제거는 근본적인 해결이 아닐 수 있음. 왜냐하면..  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Principal Component Analysis (PCA)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "PCA를 사용해 차원을 축소하고, 다중공선성을 제거한 새로운 변수로 모델을 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 Score After PCA: 0.999950731484512\n",
      "Explained Variance Ratio: [0.98406031 0.0133858 ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Standardizing the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Apply PCA (Principal Component Analysis) with 2 principal components\n",
    "# PCA is used to reduce dimensionality by transforming correlated variables into independent components.\n",
    "# Here, we use 2 principal components to retain the most significant variance while simplifying the dataset.\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Train-test split with the transformed PCA data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the regression model with PCA-transformed data\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions and evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"R^2 Score After PCA:\", r2_score(y_test, y_pred))\n",
    "\n",
    "# Output the explained variance ratio of the principal components\n",
    "# This shows how much variance is retained by each principal component.\n",
    "print(\"Explained Variance Ratio:\", pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "왜 PCA를 하기로 했는지  \n",
    "어떻게 했는지 (왜 주성분이 2개인지)  \n",
    "결과과\n",
    "한계: 근본적인 해결이 아닐 수 있음. 왜냐하면.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 Score with Ridge Regression: 0.9999515809826575\n",
      "Ridge Coefficients: [1.90755538 0.27665518]\n",
      "Mean Squared Error with Ridge Regression: 0.0010595567328021806\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Ridge regression model\n",
    "ridge = Ridge(alpha=1.0)\n",
    "ridge.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_ridge = ridge.predict(X_test)\n",
    "\n",
    "# R^2 score\n",
    "print(\"R^2 Score with Ridge Regression:\", r2_score(y_test, y_pred_ridge))\n",
    "\n",
    "# Coefficients\n",
    "print(\"Ridge Coefficients:\", ridge.coef_)\n",
    "\n",
    "# Mean squared error (MSE) comparison\n",
    "mse_ridge = mean_squared_error(y_test, y_pred_ridge)\n",
    "print(\"Mean Squared Error with Ridge Regression:\", mse_ridge)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "왜 릿지회귀를 를 하기로 했는지  \n",
    "어떻게 했는지   \n",
    "결과  \n",
    "한계: 근본적인 해결이 아닐 수 있음. 왜냐하면.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 Score with Lasso Regression: 0.9996497233047609\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# 라쏘 회귀 모델\n",
    "lasso = Lasso(alpha=0.1)  # 알파 값은 정규화 강도를 조절\n",
    "lasso.fit(X_train, y_train)\n",
    "y_pred_lasso = lasso.predict(X_test)\n",
    "\n",
    "# R^2 결과 출력\n",
    "print(\"R^2 Score with Lasso Regression:\", r2_score(y_test, y_pred_lasso))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "왜 라쏘쏘회귀를 를 하기로 했는지  \n",
    "어떻게 했는지   \n",
    "결과  \n",
    "한계: 근본적인 해결이 아닐 수 있음. 왜냐하면.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**전체 결론**\n",
    "1. **Subset Selection**:\n",
    "   - 변수 제거로 다중공선성을 완화할 수 있으나, 가장 간단한 접근법.\n",
    "   - 변수 제거는 데이터 해석에 영향을 줄 수 있음.\n",
    "\n",
    "2. **PCA**:\n",
    "   - 다중공선성을 완전히 제거하며, 모델의 설명력을 유지.\n",
    "   - 차원을 축소하므로 모델 해석력이 떨어질 수 있음.\n",
    "\n",
    "3. **Ridge Regression**:\n",
    "   - 다중공선성을 해결하면서 모든 변수를 유지.\n",
    "   - 해석 가능성이 높음.\n",
    "\n",
    "4. **Lasso Regression**:\n",
    "   - 다중공선성 해결과 변수 선택을 동시에 수행.\n",
    "   - 가장 해석 가능하고 간단한 모델을 생성할 수 있음.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **7. Predict Future**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 선형 회귀를 사용하여, 현재 (또는 과거)의 온도와 변형률을 함수로 하여 미래의 변형률을 예측하는 모델을 개발하십시오. (미래 시점에서 수집된 데이터를 사용할 수 없습니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이전 질문에서 제안한 모델과 관련하여, 예측의 정확도를 정량화하십시오.\n",
    "미래 변형률에 대해 95% 신뢰구간을 어떻게 정의할 수 있습니까?\n",
    "이를 답하기 위해, 연속된 시점에서 변형률에 영향을 미치는 노이즈가 상관되어 있을 수 있음을 고려하십시오."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "노이즈 상관성 문제를 분석하고 이것이 초래하는 결과를 설명하며, 미래 변형률에 대한 신뢰구간을 정의할 때 이 현상을 어떻게 반영할 수 있을지 논의하십시오."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다중공선성이 마지막 두 질문에서 정의된 미래 변형률 예측과 어떻게 관련되어 있는지 논의"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "StrainPrediction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
